{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU training with Brain Builder and Tensorflow \n",
    "\n",
    "Welcome! In this tutorial, you'll learn how to train a semantic segmentation model on a custom dataset tagged using BrainBuilder across multiple-gpus. We'll be going through a simple piece of code that will enable you to use Keras, Tensorflow Estimators and Tensorflow Dataset API to write highly efficient deep learning code. \n",
    "\n",
    "We'll be training a popular architecture for semantic segmentaion, UNet, to do pixelwise segmentation of people in a scene. The tutorial also points out how data tagged through BrainBuilder ties nicely with the Tensorflow Dataset API, enabling you to use the highly efficient data pipeline and carry out those deep learning experiments quickly! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "os.sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "os.sys.path.append(os.path.dirname(os.path.abspath('..')))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"  \n",
    "from utils.UNet import UNet \n",
    "from utils.TF_Dataset_Loader import TFDatasetLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the dataset and training parameters \n",
    "\n",
    "For the data directory path, you specify the folder that exported by Brain Builder and the data loader we've written for this tutorial should work out of the box for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirpath = '/home/abhishekgaur/Neurala/Brain_Builder/Multi-GPU/Data/'  # change this\n",
    "input_size = 224\n",
    "n_classes = 2\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "num_gpus = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Instatiate the dataset and the model\n",
    "The  `TFDatasetLoader` returns a TensorFlow Dataset object that is later passed as an input to the training function. Please note that the dataset loader is specific to this particular task of semantic segmentation. We do not make resizing the part of the dataset loader since Brain Builder spits out resized dataset for us. Moreover, resizing is an expensive (and mostly CPU-bound) operation and should be avoided whenever possible. \n",
    "\n",
    "The model is written using [Keras' Functional API](https://keras.io/getting-started/functional-api-guide/). Keras is a high-level API to build and train deep learning models and is user friendly, modular and easy to extend. [tf.keras](https://www.tensorflow.org/guide/keras) is TensorFlow's implementation of this API and it supports such things as eager execution, tf.data pipelines and Estimators.\n",
    "\n",
    "The model we're building is [UNet](https://arxiv.org/abs/1505.04597). Please note that our UNet implementation is slightly modified to make the training and convergence a bit faster. The model however can be easily switched with any model of your choice. \n",
    "\n",
    "We use binary crossentropy loss function and Adam optimizer for training. We'll measure the progress using pixel accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TFDatasetLoader(data_dirpath, input_size, n_classes, batch_size, num_epochs)\n",
    "model = UNet(input_size, n_classes)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create an Estimator\n",
    "\n",
    "To create a TensorFlow Estimator from a compiled Keras model, the `model_to_estimator` method is used. \n",
    "\n",
    "Advantages of using Estimators:\n",
    "\n",
    "* Estimator-based models can be run in a distributed multi-GPU environment without changing the model code.\n",
    "* Estimators simplify sharing implementations between model developers.\n",
    "* Estimators can build the graph without needing an explicit (and resonably painful) session.\n",
    "\n",
    "We will use the `tf.contrib.MirroredStrategy` paradigm wherein each worker GPU has a copy of the graph and gets a subset of the data on which it computes the local gradients. Once the local gradients are computed, each worker then waits for other workers to finish in a synchronous manner. Once all the gradients have arrived each worker averages them and updates its parameter and the next step begins. You can learn more about the Distributed TensorFlow training on [this](https://www.youtube.com/watch?v=bRMGoPqsn20) link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=num_gpus)\n",
    "config = tf.estimator.RunConfig(train_distribute=strategy)\n",
    "estimator = tf.keras.estimator.model_to_estimator(model, config=config)\n",
    "# estimator = tf.keras.estimator.model_to_estimator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 (optional): Define a class for logging time \n",
    "\n",
    "We use the `tf.train.SessionRunHook` to monitor and log the time taken by each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeHistory(tf.train.SessionRunHook):\n",
    "    def begin(self):\n",
    "        self.times = []\n",
    "    def before_run(self, run_context):\n",
    "        self.iter_time_start = time.time()\n",
    "    def after_run(self, run_context, run_values):\n",
    "        self.times.append(time.time() - self.iter_time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train and Evaluate\n",
    "\n",
    "We call the `train` and `evaluate` function on the estimator we created. In order for us to call these functions, we need to pass it a `tf.data` object which helps it preprocess the input data and batch it to feed to the network. You can look at the definiton of the dataset loader in the file `TFDatasetLoader.py`.\n",
    "\n",
    "We also pass `time_hist` class as a hook in the training function call to log the time taken by each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_hist = TimeHistory()\n",
    "\n",
    "estimator.train(lambda:dataset.imgs_input_fn(mode=\"train\"), hooks=[time_hist])\n",
    "estimator.evaluate(lambda:dataset.imgs_input_fn(mode=\"val\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(\"../Nets/PeopleUnet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "We will use our timing hook to calculate the total time of training as well as the number of images we train on per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_time =  sum(time_hist.times)\n",
    "print(\"Total time with %d GPUs:  %fseconds\" %(num_gpus, total_time))\n",
    "\n",
    "avg_time_per_batch = np.mean(time_hist.times)\n",
    "print(\"%f images/second with %d GPUs\" %((batch_size*num_gpus/avg_time_per_batch), num_gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
